
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/tutorial_02_vhls.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_tutorial_02_vhls.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_tutorial_02_vhls.py:


Vivado HLS Backend
==================

**Author**: Hongzheng Chen (hzchen@cs.cornell.edu)


In this tutorial, we will demonstrate how to leverage the new HeteroCL DSL frontend to generate
Vivado HLS code for FPGA.

Import HeteroCL
---------------
First, we import the necessary packages.

.. GENERATED FROM PYTHON SOURCE LINES 18-22

.. code-block:: default


    import heterocl as hcl
    from heterocl.ir.types import float32








.. GENERATED FROM PYTHON SOURCE LINES 23-29

Algorithm Definition
--------------------
We again define a general matrix multiplication (GEMM) in this tutorial.
However, we will make some changes to demonstrate more features of the DSL.

We can define the constants as follows, which denotes the matrix sizes:

.. GENERATED FROM PYTHON SOURCE LINES 29-32

.. code-block:: default


    M, N, K = 1024, 1024, 1024








.. GENERATED FROM PYTHON SOURCE LINES 33-48

Here, we define the main computation of the GEMM but use ``float32`` as the
data type. Notice that users can easily leverage the previously defined arguments
(e.g., ``M``, ``N``, and ``K``) to construct the matrices, and HeteroCL will
automatically captures the global variables.

Since HeteroCL has a strict type system, we need to be careful about the
data types of the variables. To initialize matrix ``C`` with all zeros, we
need to pass in a floating-point value ``0.0`` instead of an integer.

We also use the ``hcl.reduction`` API to denote the reduction axis. The
reduction axis is the loop iterator that is used to accumulate the result.
In this example, we use ``k`` as the reduction axis, which means the
computation of ``C[i, j]`` will be accumulated along the ``k`` dimension.
This annotation is necessary for later optimizations, since HeteroCL leverages
this information to generate correct intermediate buffers.

.. GENERATED FROM PYTHON SOURCE LINES 48-58

.. code-block:: default



    def gemm(A: float32[M, K], B: float32[K, N]) -> float32[M, N]:
        C: float32[M, N] = 0.0
        for i, j in hcl.grid(M, N):
            for k in hcl.reduction(K):
                C[i, j] += A[i, k] * B[k, j]
        return C









.. GENERATED FROM PYTHON SOURCE LINES 59-75

Scalar-Vector Product for GEMM
------------------------------

Next, we create a schedule for the GEMM and start to optimize the program.
We try to implement the **interleaving accumulation** technique presented in
`this paper <https://arxiv.org/abs/1805.08288>`_, which is also viewed as
the **scalar-vector product** since it changes the computation order of the
original dot-product.

.. image:: ../_static/scalar-vector-product.png
   :width: 600

.. note::

   To get more rational of this technique, please refer to the above mentioned
   paper from Torsten Hoefler's group.

.. GENERATED FROM PYTHON SOURCE LINES 75-78

.. code-block:: default


    s = hcl.customize(gemm)








.. GENERATED FROM PYTHON SOURCE LINES 79-81

We first reorder the inner reduction loop with the middle loop.
This is used to change the computation order of matrix multiplication.

.. GENERATED FROM PYTHON SOURCE LINES 81-85

.. code-block:: default


    s.reorder("k", "j")
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<1024x1024xf32>, %arg1: memref<1024x1024xf32>) -> memref<1024x1024xf32> {
        %0 = memref.alloc() {name = "C"} : memref<1024x1024xf32>
        %cst = arith.constant 0.000000e+00 : f32
        linalg.fill ins(%cst : f32) outs(%0 : memref<1024x1024xf32>)
        affine.for %arg2 = 0 to 1024 {
          affine.for %arg3 = 0 to 1024 {
            affine.for %arg4 = 0 to 1024 {
              %1 = affine.load %arg0[%arg2, %arg3] {from = "A"} : memref<1024x1024xf32>
              %2 = affine.load %arg1[%arg3, %arg4] {from = "B"} : memref<1024x1024xf32>
              %3 = arith.mulf %1, %2 : f32
              %4 = affine.load %0[%arg2, %arg4] {from = "C", to = "C"} : memref<1024x1024xf32>
              %5 = arith.addf %4, %3 : f32
              affine.store %5, %0[%arg2, %arg4] {to = "C"} : memref<1024x1024xf32>
            } {loop_name = "j"}
          } {loop_name = "k", op_name = "S_k", reduction}
        } {loop_name = "i", op_name = "S_i_j"}
        return %0 : memref<1024x1024xf32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 86-92

.. note::

   This reordering seems to be easy, but it is impossible in the old HeteroCL,
   since the previous HeteroCL directly generate reduction variables which make
   the ``j`` loop becomes imperfect, while MLIR only supports reordering perfect
   loops.

.. GENERATED FROM PYTHON SOURCE LINES 94-98

Next, we create a new buffer for the output tensor ``C``.
We provide a ``.buffer_at()`` primitive for users to quickly create a new buffer
along a specific axis. Since HeteroCL has attached all the tensors to the function,
we can directly use ``<func>.<tensor>`` to access a specific tensor in the schedule.

.. GENERATED FROM PYTHON SOURCE LINES 98-102

.. code-block:: default


    s.buffer_at(gemm.C, axis="i")
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<1024x1024xf32>, %arg1: memref<1024x1024xf32>) -> memref<1024x1024xf32> {
        %0 = memref.alloc() {name = "C"} : memref<1024x1024xf32>
        %cst = arith.constant 0.000000e+00 : f32
        linalg.fill ins(%cst : f32) outs(%0 : memref<1024x1024xf32>)
        affine.for %arg2 = 0 to 1024 {
          %1 = memref.alloc() : memref<1024xf32>
          affine.for %arg3 = 0 to 1024 {
            affine.store %cst, %1[%arg3] : memref<1024xf32>
          } {buffer, loop_name = "j_init", pipeline_ii = 1 : i32}
          affine.for %arg3 = 0 to 1024 {
            affine.for %arg4 = 0 to 1024 {
              %2 = affine.load %arg0[%arg2, %arg3] {from = "A"} : memref<1024x1024xf32>
              %3 = affine.load %arg1[%arg3, %arg4] {from = "B"} : memref<1024x1024xf32>
              %4 = arith.mulf %2, %3 : f32
              %5 = affine.load %1[%arg4] : memref<1024xf32>
              %6 = arith.addf %5, %4 : f32
              affine.store %6, %1[%arg4] : memref<1024xf32>
            } {loop_name = "j"}
          } {loop_name = "k", op_name = "S_k", reduction}
          affine.for %arg3 = 0 to 1024 {
            %2 = affine.load %1[%arg3] : memref<1024xf32>
            affine.store %2, %0[%arg2, %arg3] : memref<1024x1024xf32>
          } {buffer, loop_name = "j_back", pipeline_ii = 1 : i32}
        } {loop_name = "i", op_name = "S_i_j"}
        return %0 : memref<1024x1024xf32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 103-107

From the above generated code, we can see that HeteroCL automatically
creates an intermediate buffer ``%1`` for ``C`` and attach it inside the ``i`` loop.
Also two additional loop nested named ``j_init`` and ``j_back`` are created to
initialize and write the intermediate buffer back to output tensor.

.. GENERATED FROM PYTHON SOURCE LINES 109-110

Lastly, we pipeline the ``j`` loop in order to achieve the best performance.

.. GENERATED FROM PYTHON SOURCE LINES 110-114

.. code-block:: default


    s.pipeline("j")
    print(s.module)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    module {
      func.func @gemm(%arg0: memref<1024x1024xf32>, %arg1: memref<1024x1024xf32>) -> memref<1024x1024xf32> {
        %0 = memref.alloc() {name = "C"} : memref<1024x1024xf32>
        %cst = arith.constant 0.000000e+00 : f32
        linalg.fill ins(%cst : f32) outs(%0 : memref<1024x1024xf32>)
        affine.for %arg2 = 0 to 1024 {
          %1 = memref.alloc() : memref<1024xf32>
          affine.for %arg3 = 0 to 1024 {
            affine.store %cst, %1[%arg3] : memref<1024xf32>
          } {buffer, loop_name = "j_init", pipeline_ii = 1 : i32}
          affine.for %arg3 = 0 to 1024 {
            affine.for %arg4 = 0 to 1024 {
              %2 = affine.load %arg0[%arg2, %arg3] {from = "A"} : memref<1024x1024xf32>
              %3 = affine.load %arg1[%arg3, %arg4] {from = "B"} : memref<1024x1024xf32>
              %4 = arith.mulf %2, %3 : f32
              %5 = affine.load %1[%arg4] : memref<1024xf32>
              %6 = arith.addf %5, %4 : f32
              affine.store %6, %1[%arg4] : memref<1024xf32>
            } {loop_name = "j", pipeline_ii = 1 : i32}
          } {loop_name = "k", op_name = "S_k", reduction}
          affine.for %arg3 = 0 to 1024 {
            %2 = affine.load %1[%arg3] : memref<1024xf32>
            affine.store %2, %0[%arg2, %arg3] : memref<1024x1024xf32>
          } {buffer, loop_name = "j_back", pipeline_ii = 1 : i32}
        } {loop_name = "i", op_name = "S_i_j"}
        return %0 : memref<1024x1024xf32>
      }
    }





.. GENERATED FROM PYTHON SOURCE LINES 115-120

Codegen for Vivado HLS
----------------------
Similar to the CPU execution, we only need to change the target of the ``.build()`` function
in order to target different backends. Here, we use ``vhls`` as the target to generate
Vivado HLS code, which will returns the generated code as a string.

.. GENERATED FROM PYTHON SOURCE LINES 120-124

.. code-block:: default


    code = s.build(target="vhls")
    print(code)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    //===------------------------------------------------------------*- C++ -*-===//
    //
    // Automatically generated file for High-level Synthesis (HLS).
    //
    //===----------------------------------------------------------------------===//
    #include <algorithm>
    #include <ap_axi_sdata.h>
    #include <ap_fixed.h>
    #include <ap_int.h>
    #include <hls_math.h>
    #include <hls_stream.h>
    #include <math.h>
    #include <stdint.h>
    using namespace std;
    void gemm(
      float v0[1024][1024],
      float v1[1024][1024],
      float v2[1024][1024]
    ) {     // L507
      for (int v3 = 0; v3 < 1024; v3++) {   //
        for (int v4 = 0; v4 < 1024; v4++) { //
          v2[v3][v4] = 0.000000;    //
        }
      }
      l_S_i_j_i: for (int i = 0; i < 1024; i++) {   //
        float v6[1024];     //
        l_j_init: for (int j_init = 0; j_init < 1024; j_init++) {   //
        #pragma HLS pipeline II=1
          v6[j_init] = 0.000000;    //
        }
        l_S_k_k: for (int k = 0; k < 1024; k++) {   //
          l_j: for (int j = 0; j < 1024; j++) {     //
          #pragma HLS pipeline II=1
            float v10 = v0[i][k];   //
            float v11 = v1[k][j];   //
            float v12 = v10 * v11;  //
            float v13 = v6[j];      //
            float v14 = v13 + v12;  //
            v6[j] = v14;    //
          }
        }
        l_j_back: for (int j_back = 0; j_back < 1024; j_back++) {   //
        #pragma HLS pipeline II=1
          float v16 = v6[j_back];   //
          v2[i][j_back] = v16;      //
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 125-128

We can see that the generated code preserves the same structure as the IR, and inserts
necessary headers and pragmas for Vivado HLS. The generated code can be directly passed
to Vivado HLS to generate RTL designs.

.. GENERATED FROM PYTHON SOURCE LINES 130-133

We even provide an easy way to invoke Vivado HLS from HeteroCL. Users can simply
create a target platform, and configure the target with the ``vivado_hls`` compiler.
The ``project`` argument is used to specify the name of the Vivado HLS project folder.

.. GENERATED FROM PYTHON SOURCE LINES 133-137

.. code-block:: default


    target = hcl.Platform.xilinx_zc706
    target.config(compiler="vivado_hls", mode="debug", project="gemm.prj")








.. GENERATED FROM PYTHON SOURCE LINES 138-140

Finally, we call the ``.build()`` function with the specified ``target`` to generate
the Vivado HLS project.

.. GENERATED FROM PYTHON SOURCE LINES 140-143

.. code-block:: default


    mod = s.build(target=target)








.. GENERATED FROM PYTHON SOURCE LINES 144-162

You will see a ``gemm.prj`` folder is generated in the current directory:

- ``host.cpp``: The host (CPU) code that invokes the generated accelerator.
- ``kernel.cpp``: The generated accelerator code.
- ``run.tcl``: The Vivado HLS script that can be used to generate the Vivado HLS project.
- ``Makefile``: Defined some shorthands for compiling the project.

To run Vivado HLS, you can simply invoke the built module without passing any arguments into it.

.. note::

   You need to configure the Vivado HLS environment before running the generated code.
   We have the Vivado environment configured in the ``brg-zhang`` server, so you can directly
   ``source /work/shared/common/heterocl/vitis_2019.2_opt.sh`` to set up the environment.

.. code-block:: python

   mod()

.. GENERATED FROM PYTHON SOURCE LINES 164-199

After executing the above command, you will see the following output:

.. code-block:: python

   +-------------------+-----------------------------------+
   | HLS Version       | Vivado HLS 2019.2.1               |
   | Product family    | zynq                              |
   | Target device     | xc7z020-clg484-1                  |
   | Top Model Name    | gemm                              |
   +-------------------+-----------------------------------+
   | Target CP         | 10.00 ns                          |
   | Estimated CP      | 8.052 ns                          |
   | Latency (cycles)  | Min 1077958658; Max 1077958658    |
   | Interval (cycles) | Min 1077958659; Max 1077958659    |
   | Resources         | Type        Used    Total    Util |
   |                   | --------  ------  -------  ------ |
   |                   | BRAM_18K       2      280      1% |
   |                   | DSP48E         5      220      2% |
   |                   | FF           862   106400      1% |
   |                   | LUT         1375    53200      3% |
   +-------------------+-----------------------------------+
   +---------------+--------------+------------+---------------------+---------------+------------------+
   |               |   Trip Count |    Latency |   Iteration Latency |   Pipeline II |   Pipeline Depth |
   |---------------+--------------+------------+---------------------+---------------+------------------|
   | Loop1         |         1024 |    2099200 |                2050 |           N/A |              N/A |
   | + Loop1.1     |         1024 |       2048 |                   2 |           N/A |              N/A |
   | l_S_i_j_i     |         1024 | 1075859456 |             1050644 |           N/A |              N/A |
   | + l_j_init    |         1024 |       1024 |                 N/A |             1 |                1 |
   | + l_S_k_k_l_j |      1048576 |    1048588 |                 N/A |             1 |               14 |
   | + l_j_back    |         1024 |       1025 |                 N/A |             1 |                3 |
   +---------------+--------------+------------+---------------------+---------------+------------------+
   * Units in clock cycles

From the above output, we can clearly see that all the loops inside the GEMM kernel are pipelined
with II=1.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.131 seconds)


.. _sphx_glr_download_gallery_tutorial_02_vhls.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_02_vhls.py <tutorial_02_vhls.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_02_vhls.ipynb <tutorial_02_vhls.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
